{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdf7285-fdb9-4b19-b0a1-54631bdadb0d",
   "metadata": {},
   "source": [
    "# Coding Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d94a26-47e8-467c-b457-fe90d8d2e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as nplin\n",
    "import math\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "from scipy.special import expit\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64f71d-3f24-4c2e-a5d5-de804913dc26",
   "metadata": {},
   "source": [
    "shuffle training data and split into a training and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2f6e9-0d76-4d99-9a9e-042e1db26c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, frac=0.3, seed=None):\n",
    "    if seed is not None:\n",
    "        n_testset = math.ceil(0.3*X.shape[0])\n",
    "        n_trainset = X.shape[0] - n_testset\n",
    "        np.random.seed(seed)\n",
    "        idx = np.arange(X.shape[0])\n",
    "        idx_shuffled = np.random.permutation(idx)\n",
    "\n",
    "        test_idx = idx_shuffled[:n_testset]\n",
    "        train_idx = idx_shuffled[n_testset:]\n",
    "        X_test = X[test_idx, : ]\n",
    "        y_test = y[test_idx]\n",
    "        X_train = X[train_idx, : ]\n",
    "        y_train = y[train_idx]\n",
    "        \n",
    "        return X_test, y_test, X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06432fa0-6115-4a83-aa22-c5451938f262",
   "metadata": {},
   "source": [
    "manual ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad3dd8a-5518-4145-9e99-2e19abbeb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X_test, X_train, y_train, alpha):\n",
    "    \n",
    "    Xtr_design =  np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "    X_te_design = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "    inv = nplin.inv(Xtr_design.T @ Xtr_design + alpha * np.identity(Xtr_design.shape[1]))\n",
    "    weights = inv @ Xtr_design.T @ y_train\n",
    "\n",
    "    y_pred = X_te_design @ weights\n",
    "    \n",
    "    return weights, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ca820-1892-44c6-9286-b75d4b4a821a",
   "metadata": {},
   "source": [
    "n_fold-crossvalidation over the ridge regression parameter alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a0610-3c00-4fe2-8fff-bf884ef8d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridgeCV(X, y, n_folds, alphas):\n",
    "\n",
    "    cv_results_mse = np.zeros((n_folds, len(alphas)))\n",
    "    np.random.seed(seed=2)\n",
    "\n",
    "    idx = np.repeat(range(n_folds), math.ceil(X.shape[0]/n_folds))\n",
    "    idx_shuff = np.random.permutation(idx)  \n",
    "    idx_shuffled = idx_shuff[:X.shape[0]]\n",
    "    X_new = np.c_[idx_shuffled, X]\n",
    "    y_new = np.c_[idx_shuffled, y]\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        test_data = X_new[X_new[:,0] == i]\n",
    "        train_data = X_new[X_new[:,0] != i]\n",
    "        train_y = y_new[y_new[:,0] != i]\n",
    "        test_y = y_new[y_new[:,0] == i]\n",
    "        \n",
    "        for j in range(len(alphas)):\n",
    "            y_pred = np.empty([test_data.shape[0], len(alphas)])\n",
    "            y_pred[:,j:j+1] = ridge_regression_sklearn(test_data[:,1:], train_data[:,1:], train_y[:,1:], alphas[j])[1]\n",
    "            cv_results_mse[i:i+1:,j:j+1] = mean_squared_error(test_y[:,1:], y_pred[:,j])\n",
    "\n",
    "    return cv_results_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6b3a1d-adec-4051-9d14-35307776e56d",
   "metadata": {},
   "source": [
    "manual linear discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3945b873-07d6-4b7f-8b1f-8303acf5bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lda_weights(x, y):\n",
    "    \n",
    "    N0 = sum(y)\n",
    "    N1 = sum(y==False)\n",
    "    x0 = x[y==True]\n",
    "    x1 = x[y==False]\n",
    "    \n",
    "    m0 = (1/N0) * np.sum(x0, axis=0)\n",
    "    m1 = (1/N1) * np.sum(x1, axis=0)\n",
    "    d0 = np.array(x0-m0)\n",
    "    d1 = np.array(x1-m1)\n",
    "\n",
    "    S_w = (d0.T@d0) + (d1.T@d1)\n",
    "    mdiff = (m1-m0)\n",
    "    w_lda = nplin.inv(S_w) @ mdiff\n",
    "    \n",
    "    return m0, m1, mdiff, w_lda   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e25af-3215-4a12-8168-5d0b70084135",
   "metadata": {},
   "source": [
    "manual logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ff91c-bea5-44a0-b2d6-b409d6b26aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irls_al(X,t):\n",
    "    t = np.asmatrix(t)\n",
    "    Phi = np.c_[np.ones(X.shape[0]), X]\n",
    "    w = np.full((Phi.shape[1], 1), 0.000000001)\n",
    "    y = expit(Phi @ w)\n",
    "    acc = np.sum(y==t)/X.shape[0]\n",
    "    t_label = np.zeros(shape = X.shape[0])\n",
    "    \n",
    "    while acc<0.8:\n",
    "        d = y-(1-y)\n",
    "        d = np.array(d[:,0]).flatten()\n",
    "        R = np.diag(d)\n",
    "        w = w - nplin.inv(Phi.T @ R @ Phi) @ Phi.T @ (y-t.T)\n",
    "        y = expit(Phi @ w)\n",
    "        \n",
    "        t_label[np.array(y[:,0]>0.5).flatten()] = 1\n",
    "        t_label[np.array(y[:,0]<0.5).flatten()] = 0\n",
    "        acc = np.sum(t_label==np.array(t).flatten())/X.shape[0]\n",
    "        print(acc)\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4aac42-2abe-42b9-a4ba-972d8651f505",
   "metadata": {},
   "source": [
    "manual Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42283b04-c985-4be8-a270-9e96a5d21a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_svm(X, t, kernel, C=1.0):\n",
    "\n",
    "    t = np.array([-1 if l == 0 else 1 for l in t])\n",
    "    n_samples = len(X)\n",
    "    K = kernel(X,X)      \n",
    "    P = np.outer(t,t)*K\n",
    "    q = np.ones(n_samples)*-1\n",
    "    A = np.asmatrix(t*1.0)\n",
    "    b = np.asmatrix(0.0)\n",
    "    G = np.vstack([np.eye(n_samples),np.eye(n_samples)*-1])\n",
    "    h = np.hstack([C*np.ones(n_samples),np.zeros(n_samples)])\n",
    "\n",
    "    assert P.shape == (len(X), len(X))\n",
    "    assert len(q) == len(X)\n",
    "    assert A.shape == (1, n_samples) and A.dtype == 'float'\n",
    "    assert b.shape == (1, 1)\n",
    "    assert len(G) == 2 * len(X)\n",
    "    assert len(h) == 2 * len(X)\n",
    "\n",
    "    return solve_quadratic_program(P, q, A, b, G, h)\n",
    "\n",
    "def solve_quadratic_program(P, q, A, b, G, h):\n",
    "    P, q, A, b, G, h = [cvxopt.matrix(var) for var in [P, q, A, b, G, h]]\n",
    "    minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    lagr_mult = np.ravel(minimization['x'])\n",
    "    return lagr_mult\n",
    "\n",
    "\n",
    "def extract_parameters(X, t, kernel, lagr_mult, threshold=1e-7):\n",
    "    \n",
    "    t = np.array([-1 if l == 0 else 1 for l in t])\n",
    "    K = kernel(X,X)\n",
    "    \n",
    "    svs = (lagr_mult>threshold)*(lagr_mult<1.0)\n",
    "    sv_labels = t[svs]\n",
    "    intercept = np.mean(sv_labels - lagr_mult[svs]*sv_labels*K[svs,svs])\n",
    "\n",
    "    return lagr_mult, svs, sv_labels, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f384c-822d-4f02-999b-5ab897842f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = fit_svm(X_train, t_train, rbf_kernel)\n",
    "\n",
    "lagr_mult, svs, sv_labels, intercept = extract_parameters(X_train, t_train, rbf_kernel, alphas, threshold=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef51620-5033-4492-92eb-37dfb4d1656f",
   "metadata": {},
   "source": [
    " manual Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98193b4b-a3ec-4ed0-8570-10cb9576f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_manual(data):\n",
    "\n",
    "    cov = np.cov(data.T)\n",
    "    eig_val, eig_vec = nplin.eig(cov)\n",
    "    eig_vec = eig_vec.T\n",
    "    idx = np.argsort(eig_val)[::-1]\n",
    "    principal_components = eig_vec[idx].T\n",
    "    fraction_variance_explained = np.empty((data.shape[1],1))\n",
    "    \n",
    "    for i in range(data.shape[1]):\n",
    "        fraction_variance_explained[i] = eig_val[i]/eig_val.sum()\n",
    "\n",
    "    return fraction_variance_explained, principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f7b85-2d89-4dcf-82de-1afc6f51cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_PCs(variance_explained, principal_components, percent_variance=None):\n",
    "    \n",
    "    n = np.sum(variance_explained.cumsum()<percent_variance) \n",
    "    principal_components_kept = principal_components[:,0:n]\n",
    "    variance_explained_kept = variance_explained.cumsum()[n]\n",
    "\n",
    "    return variance_explained_kept, principal_components_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf7c89-a11f-4d01-9a21-4a15aca1959e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
